# DNN_ex01 예제입니다.  



```python

# hello


def AND_gate(x1, x2):
    w1=0.5
    w2=0.5
    b=-0.7
    result = x1*w1 + x2*w2 + b
    if result <= 0:
        return 0
    else:
        return 1
```


```python
def NAND_gate(x1, x2):
    w1=-0.5
    w2=-0.5
    b=0.7
    result = x1*w1 + x2*w2 + b
    if result <= 0:
        return 0
    else:
        return 1
```


```python
def OR_gate(x1, x2):
    w1=0.6
    w2=0.6
    b=-0.5
    result = x1*w1 + x2*w2 + b
    if result <= 0:
        return 0
    else:
        return 1
```


```python

```


```python
import numpy as np
def sigmoid(x):
    return 1. / (1. + np.exp(-x))

```


```python
def numerical_derivative(f, x):
    delta_x = 1e-4 # 0.0001
    grad = np.zeros_like(x)

    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])

    while not it.finished:
        idx = it.multi_index
        tmp_val = x[idx]
        x[idx] = float(tmp_val) + delta_x
        fx1 = f(x) # f(x+delta_x)

        x[idx] = float(tmp_val) - delta_x
        fx2 = f(x) # f(x-delta_x)
        grad[idx] = (fx1 - fx2) / (2*delta_x)

        x[idx] = tmp_val
        it.iternext()

    return grad
```


```python
class LogicGate:
    def __init__(self, gate_name, xdata, tdata):
        self.name = gate_name
        self.xdata = xdata.reshape(4,2) # 입력 데이터 초기화
        self.tdata = tdata.reshape(4,1) # 정답 데이터 초기화
        self.W = np.random.rand(self.xdata.shape[1], 1) # 가중치 W 초기화
        self.b = np.random.rand(1) # 바이어스 b 초기화
        self.learning_rate = 1e-2 # 학습률 learning rate 초기화

    def loss_func(self):
        delta = 1e-7 # log 무핚대 발산 방지
        z = np.dot(self.xdata, self.W) + self.b
        y = sigmoid(z)
        return -np.sum(self.tdata*np.log(y+delta)+(1-self.tdata)*np.log((1 - y)+delta))
    
    def loss_val(self):
        delta = 1e-7 # log 무핚대 발산 방지
        z = np.dot(self.xdata, self.W) + self.b
        y = sigmoid(z)
        return -np.sum(self.tdata*np.log(y+delta)+(1-self.tdata)*np.log((1 - y)+delta))
    
    def train(self): # 경사하강법 이용하여 W, b 업데이트
        f = lambda x : self.loss_val()
        print("Initial loss value = ", self.loss_val())
        for step in range(8001):
            self.W -= self.learning_rate * numerical_derivative(f, self.W)
            self.b -= self.learning_rate * numerical_derivative(f, self.b)
            if (step % 1000 == 0):
                print("step = ", step, "loss value = ", self.loss_val())


    def predict(self, input_data): # 미래 값 예측
        z = np.dot(input_data, self.W) + self.b
        y = sigmoid(z)
        if y > 0.5:
            result = 1
        else:
            result = 0
        return y, result
    
    # 정확도 예측 함수(추가 내용)
    def accuracy(self, test_xdata, test_tdata):
        matched_list = []
        not_matched_list = []
        for index in range(len(xdata)):
            (real_val, logical_val) = self.predict(test_xdata[index])
            if logical_val == test_tdata[index]:
                matched_list.append(index)
            else:
                not_matched_list.append(index)
        accuracy_val = len(matched_list) / len(test_xdata)

        return accuracy_val
```


```python
xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])
tdata = np.array([0, 0, 0, 1])

```


```python
AND_obj = LogicGate("AND_GATE", xdata, tdata)
AND_obj.train()

```

    Initial loss value =  5.366098585963918
    step =  0 loss value =  5.295894311755687
    step =  1000 loss value =  1.0020647362613404
    step =  2000 loss value =  0.658203277833147
    step =  3000 loss value =  0.490260690858105
    step =  4000 loss value =  0.3895360449113486
    step =  5000 loss value =  0.32239568254286965
    step =  6000 loss value =  0.2745303116645126
    step =  7000 loss value =  0.23874787515717755
    step =  8000 loss value =  0.21102867584361745



```python

# AND 논리 게이트 검증

```


```python
xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])
tdata = np.array([0, 0, 0, 1])
AND_obj = LogicGate("AND_GATE", xdata, tdata)
AND_obj.train()

test_xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])
for input_data in test_data:
    (sigmoid_val, logical_val) = AND_obj.predict(input_data)
    print(input_data, " = ", logical_val)
    
test_tdata = np.array([ 0, 0, 0, 1])
accuracy_ret = AND_obj.accuracy(test_xdata, test_tdata)
print("Accuracy => ", accuracy_ret)
```

    Initial loss value =  2.947004377606431
    step =  0 loss value =  2.925593986514458
    step =  1000 loss value =  1.0008388197255074
    step =  2000 loss value =  0.6575957344101745
    step =  3000 loss value =  0.48991605083169526
    step =  4000 loss value =  0.38931504642199943
    step =  5000 loss value =  0.3222423372533545
    step =  6000 loss value =  0.274417943870932
    step =  7000 loss value =  0.23866215826229464
    step =  8000 loss value =  0.21096123340982664
    [0 0]  =  0
    [0 1]  =  0
    [1 0]  =  0
    [1 1]  =  1
    Accuracy =>  1.0



```python
# OR 논리 게이트 검증
```


```python
xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])
tdata = np.array([0, 1, 1, 1])
OR_obj = LogicGate("OR_GATE", xdata, tdata)
OR_obj.train()
test_xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])
for input_data in test_data:
    (sigmoid_val, logical_val) = OR_obj.predict(input_data)
    print(input_data, " = ", logical_val)
print('--------------------')
test_tdata = np.array([ 0, 1, 1, 1])
accuracy_ret = OR_obj.accuracy(test_xdata, test_tdata)
print("Accuracy => ", accuracy_ret)
```

    Initial loss value =  2.220284066517067
    step =  0 loss value =  2.209713484693276
    step =  1000 loss value =  0.7428092092249503
    step =  2000 loss value =  0.4406646346949828
    step =  3000 loss value =  0.30832655065745596
    step =  4000 loss value =  0.23538271126249646
    step =  5000 loss value =  0.18962036791746234
    step =  6000 loss value =  0.15840362597458746
    step =  7000 loss value =  0.13582386948119254
    step =  8000 loss value =  0.11876913268130461
    [0 0]  =  0
    [0 1]  =  1
    [1 0]  =  1
    [1 1]  =  1
    --------------------
    Accuracy =>  1.0



```python
# • NAND 논리 게이트 검증
```


```python
xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])
tdata = np.array([1, 1, 1, 0])
NAND_obj = LogicGate("NAND_GATE", xdata, tdata)
NAND_obj.train()
test_xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])
for input_data in test_data:
    (sigmoid_val, logical_val) = NAND_obj.predict(input_data)
    print(input_data, " = ", logical_val)
print('--------------------')
test_tdata = np.array([ 1, 1, 1, 0])
accuracy_ret = NAND_obj.accuracy(test_xdata, test_tdata)
print("Accuracy => ", accuracy_ret)
```

    Initial loss value =  2.933820433138413
    step =  0 loss value =  2.9241771071066327
    step =  1000 loss value =  1.0472853656934444
    step =  2000 loss value =  0.6768527388036067
    step =  3000 loss value =  0.5006394358845021
    step =  4000 loss value =  0.3961550103594635
    step =  5000 loss value =  0.3269749329862309
    step =  6000 loss value =  0.2778791839133986
    step =  7000 loss value =  0.24129868384482372
    step =  8000 loss value =  0.21303333878152858
    [0 0]  =  1
    [0 1]  =  1
    [1 0]  =  1
    [1 1]  =  0
    --------------------
    Accuracy =>  1.0



```python
#XOR 논리 게이트 검증
```


```python
xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])
tdata = np.array([0, 1, 1, 0])
XOR_obj = LogicGate("XOR_GATE", xdata, tdata)
XOR_obj.train()
test_xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])
for input_data in test_data:
    (sigmoid_val, logical_val) = XOR_obj.predict(input_data)
    print(input_data, " = ", logical_val)
print('--------------------')
test_tdata = np.array([ 0, 1, 1, 0])
accuracy_ret = XOR_obj.accuracy(test_xdata, test_tdata)
print("Accuracy => ", accuracy_ret)
```

    Initial loss value =  3.8090372850617613
    step =  0 loss value =  3.7853127947350647
    step =  1000 loss value =  2.7728653176105382
    step =  2000 loss value =  2.7725904713268883
    step =  3000 loss value =  2.7725879688777564
    step =  4000 loss value =  2.7725879238166695
    step =  5000 loss value =  2.7725879223044965
    step =  6000 loss value =  2.7725879222426073
    step =  7000 loss value =  2.7725879222399787
    step =  8000 loss value =  2.7725879222398673
    [0 0]  =  1
    [0 1]  =  1
    [1 0]  =  1
    [1 1]  =  0
    --------------------
    Accuracy =>  0.75



```python

```


```python
import numpy as np

A = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])

print(A, "\n")
print("A.shape == ", A.shape, "\n")

it = np.nditer(A, flags=['multi_index'], op_flags=['readwrite'])
while not it.finished:
    idx = it.multi_index
    print(idx, "current value => ", A[idx])
    it.iternext()

```

    [[1 2 3 4]
     [5 6 7 8]] 
    
    A.shape ==  (2, 4) 
    
    (0, 0) current value =>  1
    (0, 1) current value =>  2
    (0, 2) current value =>  3
    (0, 3) current value =>  4
    (1, 0) current value =>  5
    (1, 1) current value =>  6
    (1, 2) current value =>  7
    (1, 3) current value =>  8



```python
A[0, 0]
```




    1




```python
# XOR 
```


```python
input_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])
s1 = [] # NAND 출력
s2 = [] # OR 출력
new_input_data = [ ] # AND 입력
final_output = [ ] # AND 출력
input_data[0]
```




    array([0, 0])




```python
for index in range(len(input_data)):
    s1 = NAND_obj.predict(input_data[index]) # NAND 출력. 
    s2 = OR_obj.predict(input_data[index]) # OR 출력
    print("s1>>\n",s1)
    print("s2>>\n",s2)
    new_input_data.append(s1[-1]) # AND 입력
    new_input_data.append(s2[-1]) # AND 입력
    (sigmoid_val, logical_val) = AND_obj.predict(np.array(new_input_data))
    final_output.append(logical_val)
    new_input_data = []
        
for index in range(len(input_data)):
    print(input_data[index], " = ", final_output[index])

```

    s1>>
     (array([0.99962039]), 1)
    s2>>
     (array([0.0645391]), 0)
    s1>>
     (array([0.93975689]), 1)
    s2>>
     (array([0.97431997]), 1)
    s1>>
     (array([0.93975868]), 1)
    s2>>
     (array([0.97434462]), 1)
    s1>>
     (array([0.08459424]), 0)
    s2>>
     (array([0.99995212]), 1)
    [0 0]  =  0
    [0 1]  =  1
    [1 0]  =  1
    [1 1]  =  0



```python

```
